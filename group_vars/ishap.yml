---
hostname: ishap

### Install packages with grog.package
package_list:
  - name: acl
  - name: corosync-qdevice
  - name: pve-headers
  - name: git
  - name: ifupdown2
  - name: libgl1
  - name: libegl1
  - name: restic
  - name: screen
  - name: sudo
  - name: wireguard

### Pip Packages
pip_packages:
  - proxmoxer

### Disks
mergerfs_mount:
  - path: /mnt/Backup

external_mount:
  - path: /media/external_hdd

parity_disks:
  - path: /mnt/parity1
    source: /dev/disk/by-id/wwn-0x50014ee20bf4b5c6-part1
    fs: ext4
    opts: defaults
    content: false

data_disks:
  - path: /mnt/disk1
    source: /dev/disk/by-id/wwn-0x50014ee20c155f4e-part1
    fs: ext4
    opts: defaults
    content: true
  - path: /mnt/disk2
    source: /dev/disk/by-id/wwn-0x50014ee2616a8078-part1
    fs: ext4
    opts: defaults
    content: true

fstab_mergerfs:
  - source: "/mnt/disk*"
    mountpoint: "/mnt/Backup"
    fs: fuse.mergerfs
    opts: "defaults,nonempty,allow_other,use_ino,cache.files=off,moveonenospc=true,category.create=mfs,dropcacheonclose=true,minfreespace=100G,fsname=mergerfs"

### Infrastructure
lxc_vlans:
  - name: Dominion
    vmid: 400
    privileged: true
    gw: 192.168.10.1
    ip: 192.168.10.50
    tag: 10
    disk: 'local-lvm:10'
    cores: 2
    memory: 512
  - name: Drone
    vmid: 401
    privileged: false
    gw: 192.168.50.1
    ip: 192.168.50.15
    tag: 50
    disk: 'local-lvm:25'
    cores: 12
    memory: 2048
    nesting: 1
    keyctl: 1

lxc_main:
  - name: Omada
    vmid: 410
    privileged: true
    gw: 192.168.1.1
    ip: 192.168.1.10
    disk: 'local-lvm:15'
    cores: 2
    memory: 2048
    nesting: 1
    keyctl: 1

### LNXLink
lnxlink_bash: true
lnxlink_temperature: true
lnxlink_diskuse: true
lnxlink_mounts: true
lnxlink_shutdown: true

### Bash Aliases
bash_docker: true
bash_autorestic: true
bash_systemctl: true
bash_apt: true

### Variables
pull_backup: false
infrastructure: false
# mergerfs_ver: 2.32.4
# nag_ver: 0.0.7

### Cronjobs
cronjobs:
  - name: Scrutiny
    job: /usr/local/bin/runitor -uuid {{ secret_hc_ishap_scrutiny }} -- docker exec scrutiny /opt/scrutiny/bin/scrutiny-collector-metrics run > /dev/null 2>&1
    user: "{{ main_username }}"
    minute: 0
    hour: '*/6'
  - name: Restic Prune
    job: /usr/local/bin/runitor -uuid {{ secret_hc_restic_prune_ishap }} -- /usr/local/bin/autorestic forget -a -- prune
    user: "{{ main_username }}"
    minute: 0
    hour: 1
    weekday: 1
  - name: Restic Check
    job: /usr/local/bin/runitor -uuid {{ secret_hc_restic_check_ishap }} -- /usr/local/bin/autorestic check
    user: "{{ main_username }}"
    minute: 30
    hour: 3
    day: 1
  - name: Media Backup (Docker)
    job: /usr/local/bin/runitor -uuid {{ secret_hc_restic_backup_ishap_docker }} -- /usr/local/bin/autorestic backup -l docker -c /home/{{ main_username }}/.autorestic.yml
    user: root
    minute: 45
    hour: 23
  - name: Media Backup (Everything Else)
    job: /usr/local/bin/runitor -uuid {{ secret_hc_restic_backup_ishap_all }} -- /usr/local/bin/autorestic backup -l config -l nvr -c /home/{{ main_username }}/.autorestic.yml
    user: root
    minute: 15
    hour: 1
    weekday: 0,3,5
  - name: Run Trim
    job: /usr/local/bin/runitor -uuid {{ secret_hc_ishap_trim }} -- /bin/bash /home/{{ main_username }}/trim.sh
    user: root
    minute: 0
    hour: 10
    weekday: 1
  - name: CPU Governor
    job: "echo 'powersave' | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor"
    user: root
    special_time: reboot

### Autorestic Config
autorestic_config_user: "{{ main_username}}"
autorestic_user_directory: /home/"{{ main_username}}"
autorestic_run_check: false
autorestic_config_yaml:
  version: 2
  backends:
    local_docker:
      type: rest
      path: 'http://192.168.1.5:8500/ishap'
      key: "{{ secret_restic_repo_password }}"
      rest:
        user: "{{ secret_restic_rest_user }}"
        password: "{{ secret_restic_rest_password }}"
    local_config:
      type: rest
      path: 'http://192.168.1.5:8500/config_files'
      key: "{{ secret_restic_repo_password }}"
      rest:
        user: "{{ secret_restic_rest_user }}"
        password: "{{ secret_restic_rest_password }}"
    local_nvr:
      type: rest
      path: 'http://192.168.1.5:8500/nvr'
      key: "{{ secret_restic_repo_password }}"
      rest:
        user: "{{ secret_restic_rest_user }}"
        password: "{{ secret_restic_rest_password }}"
    b2_docker:
      type: b2
      path: "{{ secret_restic_b2_bucket }}:/ishap"
      key: "{{ secret_restic_repo_password }}"
      env:
        - B2_ACCOUNT_ID: "{{ secret_restic_b2_account_id }}"
        - B2_ACCOUNT_KEY: "{{ secret_restic_b2_account_key }}"
    b2_config:
      type: b2
      path: "{{ secret_media_b2_bucket }}:/config_files"
      key: "{{ secret_restic_repo_password }}"
      env:
        - B2_ACCOUNT_ID: "{{ secret_media_b2_account_id }}"
        - B2_ACCOUNT_KEY: "{{ secret_media_b2_account_key }}"
      options:
        backup:
          exclude:
            - '*.iso'
            - '*.bin'
            - '*.img'
    b2_nvr:
      type: b2
      path: "{{ secret_restic_b2_bucket }}:/nvr"
      key: "{{ secret_restic_repo_password }}"
      env:
        - B2_ACCOUNT_ID: "{{ secret_restic_b2_account_id }}"
        - B2_ACCOUNT_KEY: "{{ secret_restic_b2_account_key }}"
  locations:
    docker:
      from: '/home/{{ main_username }}/docker'
      to:
        - local_docker
        - b2_docker
      options:
        forget:
          keep-daily: 1
          keep-weekly: 8
          keep-monthly: 4
    config:
      from: '/mnt/Backup/config'
      to:
        - local_config
        - b2_config
      options:
        forget:
          keep-daily: 1
          keep-weekly: 8
          keep-monthly: 4
    nvr:
      from: '/mnt/Backup/NVR'
      to:
        - local_nvr
        - b2_nvr
      options:
        forget:
          keep-daily: 1
          keep-weekly: 8
          keep-monthly: 4

### Snapraid Config
snapraid_force_install: false
snapraid_install: true
snapraid_runner: true
snapraid_data_disks:
  - path: /mnt/disk1
    content: true
  - path: /mnt/disk2
    content: true
snapraid_parity_disks:
  - path: /mnt/parity1
    content: true
snapraid_bin_path: /usr/local/bin/snapraid
snapraid_runner_email_address_from: "{{ secret_snapraid_email_from }}"
snapraid_runner_email_address_to: "{{ secret_snapraid_email_to }}"
snapraid_runner_email_address: "{{ secret_snapraid_email_address }}"
snapraid_runner_email_pass: "{{ secret_snapraid_email_pass }}"
snapraid_runner_smtp_host: "{{ secret_snapraid_smtp_host }}"
snapraid_runner_email_subject: "[SnapRAID] Ishap Status Report"
snapraid_runner_smtp_port: 465
snapraid_runner_use_ssl: true
snapraid_runner_touch: false
snapraid_runner_command: "python3 {{ snapraid_runner_bin }} -c {{ snapraid_runner_conf }}"
snapraid_runner_cron_jobs:
  - {job: '/usr/local/bin/runitor -uuid {{ secret_hc_ishap_snapraid }} -- {{ snapraid_runner_command }}', name: 'snapraid_runner', weekday: '*', hour: '04'}
snapraid_config_excludes:
  - "*.unrecoverable"
  - "/tmp/"
  - "/lost+found/"
  - "downloads/"
  - "appdata/"
  - "*.!sync"
  - ".AppleDouble"
  - "._AppleDouble"
  - ".DS_Store"
  - "._.DS_Store"
  - ".Thumbs.db"
  - ".fseventsd"
  - ".Spotlight-V100"
  - ".TemporaryItems"
  - ".AppleDB"
  - ".Trash*"

### Docker-Compose with ironicbadger.docker_compose_generator
appdata_path: "/home/{{ main_username }}/docker"
containers:
  ###
  - service_name: gluetun
    container_name: gluetun
    active: true
    image: qmcgaw/gluetun:v3.39.0
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
    volumes:
      - "{{ appdata_path }}/gluetun:/gluetun"
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      - VPN_SERVICE_PROVIDER=mullvad
      - VPN_TYPE=wireguard
      - WIREGUARD_PRIVATE_KEY={{ secret_mullvad_key }}
      - WIREGUARD_ADDRESSES={{ secret_mullvad_url }}
      - SERVER_CITIES={{ secret_mullvad_cities }}
      - DNS_ADDRESS=192.168.1.11
      - TZ=America/New_York
      - UPDATER_PERIOD=24h
    include_global_env_vars: false
  ###
  - service_name: tailscale
    container_name: tailscale
    active: true
    image: tailscale/tailscale:v1.72.1
    restart: unless-stopped
    network_mode: "service:gluetun"
    cap_add:
      - net_admin
      - sys_module
    volumes:
      - "{{ appdata_path }}/tailscale:/var/lib/tailscale"
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      - TS_HOSTNAME=ishap
      - TS_AUTHKEY={{ secret_ishap_tailscale_key }}
      - TS_EXTRA_ARGS= --advertise-exit-node --accept-routes
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=true
    include_global_env_vars: false
  ###
  - service_name: docker-proxy
    container_name: docker-proxy
    active: true
    image: ghcr.io/linuxserver/socket-proxy:2.8.7
    restart: unless-stopped
    ports:
      - 2375:2375
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - LOG_LEVEL=err
      - BUILD=1
      - COMMIT=1
      - CONFIGS=1
      - CONTAINERS=1
      - DISTRIBUTION=1
      - EXEC=1
      - IMAGES=1
      - INFO=1
      - NETWORKS=1
      - NODES=1
      - PLUGINS=1
      - SERVICES=1
      - SESSSION=1
      - POST=1
    include_global_env_vars: false
  ###
  - service_name: portainer_agent
    container_name: portainer_agent
    active: true
    image: portainer/agent:2.20.3
    restart: always
    ports:
      - 9001:9001
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/volumes:/var/lib/docker/volumes
    environment:
      - AGENT_SECRET={{ secret_portainer_key }}
    include_global_env_vars: false
  ###
  - service_name: scrutiny
    container_name: scrutiny
    active: true
    image: ghcr.io/analogj/scrutiny:v0.8.1-omnibus
    restart: unless-stopped
    privileged: true
    ports:
      - 8080:8080
    volumes:
      - "{{ appdata_path }}/scrutiny:/opt/scrutiny/config"
      - /run/udev:/run/udev:ro
    devices:
      ### THIS IS LIKELY UNNCESSARY WITH THE PRIVILEGED FLAG
      - /dev/sda:/dev/sda
      - /dev/sdb:/dev/sdb
      - /dev/sdc:/dev/sdc
      - /dev/sdd:/dev/sdd
      - /dev/sde:/dev/sde
      - /dev/sdf:/dev/sdf
      - /dev/nvme0:/dev/nvme0
    cap_add:
      - SYS_RAWIO
      - SYS_ADMIN
    include_global_env_vars: false
  ###
  - service_name: restic-server
    container_name: restic-server
    active: true
    image: restic/rest-server:0.13.0
    restart: unless-stopped
    ports:
      - 8500:8000
    volumes:
      - /mnt/Backup/restic:/data
    include_global_env_vars: false
